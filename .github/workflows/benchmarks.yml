name: Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Detect benchmarks
        id: detect
        shell: bash
        run: |
          set -euo pipefail
          if find . -type f -path '*/benches/*.rs' -print -quit | grep -q .; then
            echo "has_benches=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_benches=false" >> "$GITHUB_OUTPUT"
            echo "No benches found; skipping benchmark job steps."
          fi
      
      - name: Install Rust toolchain
        if: ${{ steps.detect.outputs.has_benches == 'true' }}
        uses: dtolnay/rust-toolchain@stable
      
      - name: Cache cargo
        if: ${{ steps.detect.outputs.has_benches == 'true' }}
        uses: actions/cache@v5
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
      
      - name: Run benchmarks
        if: ${{ steps.detect.outputs.has_benches == 'true' }}
        shell: bash
        run: |
          set -euo pipefail
          cargo bench --workspace 2>&1 | tee output.txt

      - name: Generate benchmark JSON (Criterion)
        if: ${{ steps.detect.outputs.has_benches == 'true' }}
        shell: bash
        run: |
          set -euo pipefail
          python3 - << 'PY'
          import glob
          import json
          import os

          paths = sorted(glob.glob('target/criterion/*/new/estimates.json'))
          results = []

          for path in paths:
              bench_name = os.path.basename(os.path.dirname(os.path.dirname(path)))
              with open(path, 'r', encoding='utf-8') as f:
                  data = json.load(f)

              mean = data.get('mean', {})
              point = mean.get('point_estimate')
              if point is None:
                  continue

              entry = {
                  'name': bench_name,
                  'unit': 'ns',
                  'value': point,
              }

              ci = mean.get('confidence_interval', {})
              lb = ci.get('lower_bound')
              ub = ci.get('upper_bound')
              if lb is not None and ub is not None:
                  entry['range'] = str(ub - lb)

              results.append(entry)

          with open('benchmark.json', 'w', encoding='utf-8') as f:
              json.dump(results, f)

          print(f"Wrote {len(results)} benchmarks to benchmark.json")
          PY

      - name: Detect benchmark results
        id: results
        if: ${{ steps.detect.outputs.has_benches == 'true' }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -s benchmark.json ] && [ "$(cat benchmark.json)" != "[]" ]; then
            echo "has_results=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_results=false" >> "$GITHUB_OUTPUT"
            echo "No benchmark results found; skipping store step."
          fi
      
      - name: Store benchmark result
        if: ${{ steps.detect.outputs.has_benches == 'true' && steps.results.outputs.has_results == 'true' && github.event_name != 'pull_request' }}
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: strand criterion
          tool: 'customSmallerIsBetter'
          output-file-path: benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Show alert with commit comment on detecting possible performance regression
          alert-threshold: '120%'
          comment-on-alert: true
          fail-on-alert: false

  # Compare with MongoDB and PostgreSQL
  comparison:
    name: Comparative Benchmarks
    runs-on: ubuntu-latest
    services:
      mongodb:
        image: mongo:7.0
        ports:
          - 27017:27017
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v4

      - name: Detect comparison benchmark
        id: detect
        shell: bash
        run: |
          set -euo pipefail
          if find . -type f -path '*/benches/comparison.rs' -print -quit | grep -q .; then
            echo "has_comparison=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_comparison=false" >> "$GITHUB_OUTPUT"
            echo "No comparison bench found; skipping comparative benchmark steps."
          fi
      
      - name: Install Rust toolchain
        if: ${{ steps.detect.outputs.has_comparison == 'true' }}
        uses: dtolnay/rust-toolchain@stable
      
      - name: Run comparative benchmarks
        if: ${{ steps.detect.outputs.has_comparison == 'true' }}
        run: cargo bench --workspace --bench comparison
      
      - name: Generate comparison report
        if: ${{ steps.detect.outputs.has_comparison == 'true' }}
        run: |
          echo "# Benchmark Comparison Results" > comparison.md
          cat target/criterion/comparison/report/index.html >> comparison.md
      
      - name: Upload comparison report
        if: ${{ steps.detect.outputs.has_comparison == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison
          path: comparison.md